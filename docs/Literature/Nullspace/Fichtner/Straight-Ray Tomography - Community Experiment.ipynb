{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Straight-ray tomography - Community Experiment\n",
    "\n",
    "Monday morning, 7:03:51 am. You come running into your office. Breathless. Your boss is already there, sitting relaxed on your desk, looking at you with this slightly sadistic smile. For being 231 seconds too late, he tasks you with the most boring of all jobs: Picking arrival times in the 231 seismic recordings that your company just acquired somewhere in the middle of nowhere. Subsequently, you are ordered to perform a traveltime tomography in order to see if there is anything interesting to discover underground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import some Python packages\n",
    " \n",
    "Being a Python aficionado, you know how to tackle this, and you quickly start by importing a couple of Python packages for matrix-vector operations, for plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Python packages.\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'./utils')  # This contains functions to compute G.\n",
    "from grid import *\n",
    "from straight_ray_tracer import *\n",
    "\n",
    "# Set some parameters to make plots nicer.\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Measuring traveltimes\n",
    "\n",
    "So you get to work, picking traveltimes of P waves in somewhat noisy recordings, displayed on your hopelessly outdated computer screen. Click, click, click, ..., click. Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all traveltime observations.\n",
    "d_obs_1=np.load('d_obs_1.npy')\n",
    "\n",
    "# Plot traveltime observations.\n",
    "plt.plot(d_obs_1, 'kx')\n",
    "plt.ylabel('travel time [s]')\n",
    "plt.xlabel('ray path index')\n",
    "plt.title('traveltime')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measuring more traveltimes\n",
    "\n",
    "Despite being a little monotonous, you somehow like this traveltime picking job. It reminds you of these wonderfully simple computer games that you used to play many years ago; and after all, it is light work. So, you decide to repeat this procedure a couple of times. Click, click, click, ..., until your boss peeks into your office: \n",
    "\n",
    "\"What the hell ...?!?!\"\n",
    "\n",
    "So, you stop at this point where you have 10 repeated traveltime picks of the whole dataset, and you display what you got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all traveltime observations.\n",
    "d_obs_1=np.load('d_obs_1.npy')\n",
    "d_obs_2=np.load('d_obs_2.npy')\n",
    "d_obs_3=np.load('d_obs_3.npy')\n",
    "d_obs_4=np.load('d_obs_4.npy')\n",
    "d_obs_5=np.load('d_obs_5.npy')\n",
    "d_obs_6=np.load('d_obs_6.npy')\n",
    "d_obs_7=np.load('d_obs_7.npy')\n",
    "d_obs_8=np.load('d_obs_8.npy')\n",
    "d_obs_9=np.load('d_obs_9.npy')\n",
    "d_obs_10=np.load('d_obs_10.npy')\n",
    "\n",
    "# Compute mean traveltimes.\n",
    "d_obs_mean=(d_obs_1+d_obs_2+d_obs_3+d_obs_4+d_obs_5+d_obs_6+d_obs_7+d_obs_8+d_obs_9+d_obs_10)/10.0\n",
    "\n",
    "print(3.0*60.0)\n",
    "\n",
    "# Plot mean traveltimes.\n",
    "plt.plot(d_obs_mean,'kx')\n",
    "plt.ylabel('travel time [s]')\n",
    "plt.xlabel('ray path index')\n",
    "plt.title('mean traveltimes')\n",
    "plt.show()\n",
    "\n",
    "# Plot traveltime variations relative to the mean.\n",
    "plt.plot(d_obs_1-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_2-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_3-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_4-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_5-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_6-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_7-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_8-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_9-d_obs_mean, 'kx')\n",
    "plt.plot(d_obs_10-d_obs_mean, 'kx')\n",
    "plt.ylabel('travel time [s]')\n",
    "plt.xlabel('ray path index')\n",
    "plt.title('traveltime variations w.r.t mean')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean traveltimes appear reasonable to you. They mostly reflect the length of the ray paths from source to receiver. The traveltime variations with respect to the mean contain some interesting features. Some traveltimes seem to deviate a lot from the mean. They might be outliers, but who knows? And what is an outlier anyway? For measurement indices between around 120 and 170, you observe that one family of traveltime picks is consistently above the mean, while another family of picks is below the mean. Maybe this is because the P wave could not be identified unambiguously. Therefore, in some of the cases, you may have accidentally picked another arrival.\n",
    "\n",
    "**Exercise 1**: Based on the repeated traveltime measurements, build *one* dataset that you will later use for the inversion. This dataset could be the mean of all traveltimes, one specific dataset among the 10 different ones, or something else that you find meaningful. (Of course, you could, in priciple, also invert different datasets in order to produce different tomographic models. However, your boss is simple-minded, and showing him more than 1 model may stress his mental abilities too much.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_obs = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical setup of the inversion\n",
    "\n",
    "Having decided what you consider to be *the* dataset, you define the most basic geometric input for the inversion, including the dimensions of the model domain, as well as the positions of sources and receivers. This setup simulates a cross-hole tomography where sources are on one side of the domain, and receivers are on the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the numerical grid. ---------------------------------------------\n",
    "dimension=2 # Here we only consider 2D problems anyway.\n",
    "x_min=0.0 # Minimum x-coordinate  \n",
    "y_min=0.0 # Minimum y-coordinate\n",
    "dx=2.5 # Grid spacing in x-direction\n",
    "dy=2.5 # Grid spacing in y-direction\n",
    "Nx=20.0 # Number of grid points in x-direction\n",
    "Ny=20.0 # Number of grid points in y-direction\n",
    "g = grid(dimension, [x_min,y_min], [dx,dy], np.array([Nx,Ny]))\n",
    "\n",
    "# Sources and receivers. -------------------------------------------------\n",
    "src_locations = np.array([ 0.0 * np.ones((11,)), np.linspace(0,50,11)])\n",
    "rec_locations = np.array([ 50.0 * np.ones((21,)), np.linspace(0,50,21)])\n",
    "\n",
    "sources, receivers = get_all_to_all_locations(src_locations, rec_locations)\n",
    "plot_rays(sources, receivers, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute forward matrix G\n",
    "\n",
    "Knowing source and receiver positions, and the setup of the domain, you compute the forward modelling matrix **G** that connects a slowness model **m** to a synthetic data vector **d** via **d**=**Gm**. In addition to computing **G**, you also visualise the ray density and the entries of **G**. Obviously, the ray density is rather uneven, and **G** is pretty sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute G and measure how long that takes.\n",
    "tic = time.clock()\n",
    "G = create_forward_operator(sources, receivers, g)\n",
    "toc = time.clock()\n",
    "\n",
    "# Print some statistics of G.\n",
    "print('Time elapsed:      {:10.4f} s'.format(toc-tic))\n",
    "print('Matrix shape:            ', G.shape)\n",
    "print('Data points:             ', G.shape[0])\n",
    "print('Unknowns in model space: ', G.shape[1])\n",
    "print('Non-zero entries:        ', G.count_nonzero())\n",
    "print('Ratio of non-zeros: {:10.4f} %'.format(100 * G.count_nonzero() / (G.shape[0] * G.shape[1])))\n",
    "\n",
    "# Plot ray density and entries of G.\n",
    "plot_ray_density(G,g)\n",
    "\n",
    "print('Sparsity pattern of the forward matrix:')\n",
    "plt.spy(G, markersize=2)\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.xlabel('model space index')\n",
    "plt.ylabel('data space index')\n",
    "plt.title(r'non-zero entries of $\\mathbf{G}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design prior covariance matrices\n",
    "\n",
    "You quickly realise that $\\mathbf{G}^T\\mathbf{G}$ is not invertible, which forces you to regularise the inverse problem. For this, you have three tuning parameters at your disposition: the prior standard deviation of the traveltime measurements, $\\sigma_D$, the prior standard deviation of the model parameters, $\\sigma_M$, and the correlation (or smoothing) length $\\lambda$. \n",
    "\n",
    "**Exercise 2**: Find suitable values for the regularisation parameters. (This is obviously a subjective matter. You may need to run the inversion for different choices of the parameters in order to somehow make a reasonable decision.)\n",
    "\n",
    "**Note**: The prior mean model is here assumed to be known. In reality, this is of course not the case.\n",
    "\n",
    "**Note**: The correlation length cannot be chosen arbitrarily. For too large values, the prior model covariance $\\mathbf{C}_M$ is not invertible or at least poorly conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prior covariance parameters. -------------------------------------------\n",
    "sigma_d = ???         # Data standard deviation.\n",
    "sigma_m = ???         # Prior model variance (damping).\n",
    "correlation_length = ??? # Smoothing.\n",
    "\n",
    "\n",
    "# Prior data covariance matrix. ------------------------------------------\n",
    "Cd = sigma_d**2 * scipy.sparse.eye(len(d_obs))\n",
    "Cd_inv = 1 / sigma_d**2 * scipy.sparse.eye(len(d_obs))\n",
    "\n",
    "# Prior model covariance matrix. -----------------------------------------\n",
    "Cm = g.get_gaussian_prior(correlation_length)\n",
    "Cm *= sigma_m\n",
    "Cm_inv = scipy.sparse.linalg.inv(Cm)\n",
    "\n",
    "\n",
    "# Prior model. -----------------------------------------------------------\n",
    "vp = 3000.0 * np.ones(g.npoints) \n",
    "m_prior = (1/vp).ravel()\n",
    "plot_model(m_prior, g, 'prior model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solve inverse problem\n",
    "\n",
    "You are now equipped with all ingredients needed to solve the inverse problem. For this, you compute the inverse of the Hessian of the least-squares misfit functional, $\\mathbf{C}_M^{-1}+\\mathbf{G}^T \\mathbf{C}_D^{-1} \\mathbf{G}$, which is equal to the posterior covariance, $\\mathbf{\\tilde{C}}_M$.\n",
    "\n",
    "**Exercise 3**: Run the inversion and discuss the results in the context of the subjective choices you made before. Think about the effect of your chosen dataset on your solution. Furthermore, consider your choices of the regularisation parameters and how these influence the estimated model. How could you better explore the range of regularisation parameters? Are there any physical intuitions that could guide your otherwise ad hoc choices of their values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hessian ----------------------------------------------------------------\n",
    "H = G.T * Cd_inv * G + Cm_inv\n",
    "\n",
    "# Posterior covariance ---------------------------------------------------\n",
    "Cm_post = scipy.sparse.linalg.inv(H);\n",
    "\n",
    "# Posterior mean. --------------------------------------------------------\n",
    "m_est = Cm_post * (G.T * Cd_inv * d_obs + Cm_inv * m_prior)\n",
    "d_est = G * m_est\n",
    "d_prior = G * m_prior\n",
    "\n",
    "# Resolution matrix. -----------------------------------------------------\n",
    "R = np.identity(g.npoints[0]*g.npoints[1]) - Cm_inv*Cm_post\n",
    "plt.matshow(R, cmap=plt.cm.get_cmap('seismic'))\n",
    "plt.clim(-0.4,0.4)\n",
    "plt.colorbar()\n",
    "plt.title('resolution matrix')\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.show()\n",
    "\n",
    "print('number of effectively resolved model parameters: %f' % np.trace(R))\n",
    "\n",
    "# Plot. ------------------------------------------------------------------\n",
    "plot_model(m_est, g, 'reconstructed slowness')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
